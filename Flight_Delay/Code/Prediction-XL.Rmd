---
title: "ANLY511 project"
author: "Xueyan Liu"
output:
  word_document: default
  pdf_document: default
  html_document:
    df_print: paged
---

```{r}
library(nycflights13)

AL=nycflights13::airlines
Ap=nycflights13::airports
fl=nycflights13::flights
pl=nycflights13::planes
wt=nycflights13::weather
```

```{r}
AL
set.seed(101)
```

# data processing 
```{r}
library(dplyr)
# define delay flight 
# Departure and arrival delays, in minutes. Negative number represent early departures/arrivals
# Mark delay and cancellation as 1, otherwise 0 
fl$arr_label[fl$arr_delay <= 5] = 0 
fl$arr_label[fl$arr_delay > 5] = 1   # delay
fl$dep_label[fl$dep_delay <= 5] = 0
fl$dep_label[fl$dep_delay > 5] = 1   # delay


#label the time period
wt$label[wt$hour >=5 & wt$hour <= 10] = 'morning'
wt$label[wt$hour >=11 & wt$hour <= 16] = 'Afternoon'
wt$label[wt$hour >=17 & wt$hour <= 22] = 'Evening'
wt$label[wt$hour >=0 & wt$hour <= 4 | wt$hour == 23] = 'Late Night'


# set cancellation flight as 100
fl$dep_delay[is.na(fl$dep_delay)] <-  100
fl$dep_delay[is.na(fl$arr_delay)] <-  100
fl$arr_delay[is.na(fl$arr_delay)] <-  100
fl$arr_delay[is.na(fl$dep_delay)] <-  100



# compute delay prob for each carrier
print (fl %>% group_by(carrier) %>% summarize(prop.depdelay = mean(dep_label==1),prop.arrdelay = mean(arr_label==1))) 

# compute delay prob for each airport
print (fl %>% group_by(origin) %>% summarize(prop.depdelay = mean(dep_label==1),prop.arrdelay = mean(arr_label==1))) 

# compute delay prob for each carrier
print (fl %>%  summarize(prop.depdelay = mean(dep_label==1),prop.arrdelay = mean(arr_label==1))) 

# compute delay prob for each airport
print (fl %>% group_by(origin) %>% summarize(prop.depdelay = mean(dep_label==1),prop.arrdelay = mean(arr_label==1))) 

# compute delay prob for each month
print (fl %>% group_by(month) %>% summarize(prop.depdelay = mean(dep_label==1),prop.arrdelay = mean(arr_label==1))) 

```

```{r}
# function to compute the accuracy by  confusion matrix 
accuracy = function(table){
  accuracy =(table[1]+table[4])/sum(table) 
  return(accuracy)}

## label = time period label
df = merge(x=fl,y=wt, by=c("origin","time_hour")) # join the dataset 
df = df[,c("dep_delay",'dep_label',"arr_delay",'arr_label',"origin",
           'carrier','label','distance', "temp","dewp","humid",
           "wind_speed","precip","pressure","visib",'month.x')]

# correct the data type
df$carrier = as.factor(df$carrier)
df$origin = as.factor(df$origin)
df$label = as.factor(df$label)
df$month = as.factor(df$month.x)
```

```{r}
###################################################################
# logistics regression model 1
###################################################################

lg_df = df[,c("dep_label","origin",'carrier', 'distance',"temp","dewp",
              "humid","wind_speed", "precip","pressure","visib",'label')]

# split to trainning data / testing data
smp_siz = floor(0.75*nrow(lg_df))  
train_ind_lg = sample(seq_len(nrow(lg_df)),size = smp_siz)
training_lg = lg_df[train_ind_lg,] 
testing_lg = lg_df[-train_ind_lg,] 

# reg 
reg = glm(dep_label~.,data = training_lg,family = binomial)
summary(reg)
# get prediction
pred.step <- predict(reg, newdata = testing_lg, type = "response")
pred.class <- factor(ifelse(pred.step >=0.5, 1, 0))
table(pred.class, testing_lg$dep_label)
cat('accuracy of model 1',accuracy(table(pred.class, testing_lg$dep_label)),'\n') 
#fourfoldplot(table(pred.class, testing_lg$dep_label))

##ROC
library(ROCR)
pred <- prediction(pred.step ,testing_lg$dep_label)
perf <- performance(pred,"tpr","fpr")
plot(perf,colorize=TRUE,main='ROC of Logistic Regression M1')
lines(x = c(0,1), y = c(0,1))

# R^2
# ref:https://thestatsgeek.com/2014/02/08/r-squared-in-logistic-regression/
nullmod <- glm(dep_label~1, data =training_lg ,family="binomial")
R2_lg1 = 1-logLik(reg)/logLik(nullmod)
cat('R squared of log_reg model 1',R2_lg1 )
```
Number of Fisher Scoring iterations: 4 =  Fisherâ€™s Scoring Algorithm needed 4 iterations(newton method) to perform the fit.
The Akaike Information Criterion (AIC) provides a method for assessing the quality of your model through comparison of related models. after removing the unimportant variables, the models perform better.

```{r}
###################################################################
# logistics regression model 2
###################################################################

lg_df2 =df[,c("dep_label","origin",'carrier', 'distance',"temp","dewp","month",
              "humid","wind_speed", "precip","pressure","visib",'label')]

lg_df2$dewp = NULL

# split to trainning data / testing data
smp_siz2 = floor(0.75*nrow(lg_df2))  
train_ind_lg2 = sample(seq_len(nrow(lg_df2)),size = smp_siz2)
training_lg2 = lg_df2[train_ind_lg2,] 
testing_lg2 = lg_df2[-train_ind_lg2,] 

# reg 
reg2 = glm(dep_label~.,data = training_lg2,family = binomial)
summary(reg2)
# get prediction
pred.step2 <- predict(reg2, newdata = testing_lg2, type = "response")
pred.class2 <- factor(ifelse(pred.step2 >=0.5, 1, 0))
table(pred.class2, testing_lg2$dep_label)
cat('accuracy of model 2',accuracy(table(pred.class2, testing_lg2$dep_label)),'\n')


##ROC
library(ROCR)
pred2 <- prediction(pred.step2 ,testing_lg2$dep_label)
perf2 <- performance(pred2,"tpr","fpr")
plot(perf2,col='coral',main='ROC of Logistic Regression M2')
plot(perf,col='deepskyblue',main='ROC of Logistic Regression M1',add=TRUE)
lines(x = c(0,1), y = c(0,1))
legend(0,1, legend=c("Logistic Regression M1","Logistic Regression M2"),
       col=c("deepskyblue", "coral"), lty=1:1, cex=0.8)
##R^2
nullmod2 <- glm(dep_label~1, data = training_lg2,family="binomial")
R2_lg2 = 1-logLik(reg2)/logLik(nullmod2)
cat('R squared of log_reg model 2',R2_lg2)


```

```{r}
boxplot(reg2$residuals,reg$residuals,names=c('Model 1','Model 2'),main = 'Boxplot of residuals',col=c('#0099CC','#FF3333'))
```


```{r}
library('naivebayes')
df_quar = df[,c("dep_label","origin",'carrier', 'distance',"temp","dewp","month",
              "humid","wind_speed", "precip","pressure","visib",'label')]

df_quar$dep_label = as.factor(df_quar$dep_label)
df_quar$dewp = NULL

df_quar = df_quar[complete.cases(df_quar),]
# Binning Temperature 
df_quar$quartile = with(df_quar,cut(temp,breaks=quantile(temp, probs=seq(0,1,by=0.25)),
                                    labels=c(1,2,3,4),include.lowest=TRUE))
df_quar$temp =  df_quar$quartile  
# Humidity
df_quar$quartile = with(df_quar,cut(humid,breaks=quantile(humid, probs=seq(0,1,by=0.25)),
                                    labels=c(1,2,3,4),include.lowest=TRUE))
df_quar$humid =  df_quar$quartile 
# wind_speed
df_quar$quartile = with(df_quar,cut(wind_speed,breaks=quantile(wind_speed, probs=seq(0,1,by=0.25)),
                                    labels=c(1,2,3,4),include.lowest=TRUE))
df_quar$wind_speed  =  df_quar$quartile 
# Precipitation
df_quar$quartile = with(df_quar,cut(precip,breaks=c(-Inf,seq(from =0.005, to = 1.2, by=0.1),Inf),labels=seq(from =1, to = 13, by=1),include.lowest=TRUE))
df_quar$precip  =  df_quar$quartile 
# pressure
df_quar$quartile = with(df_quar,cut(pressure,breaks=quantile(pressure, probs=seq(0,1,by=0.25)),
                                    labels=c(1,2,3,4),include.lowest=TRUE))
df_quar$pressure  =  df_quar$quartile 
# Visibility
df_quar$quartile = with(df_quar,cut(visib,breaks=c(-Inf,seq(from =0, to = 1, by=0.25),seq(from =2, to = 10, by=3),Inf), labels=seq(from =1, to = 9, by=1),include.lowest=TRUE))
df_quar$visib  =  df_quar$quartile 
# distance
df_quar$quartile = with(df_quar,cut(distance,breaks=quantile(distance, probs=seq(0,1,by=0.25)),
                                    labels=c(1,2,3,4),include.lowest=TRUE))
df_quar$distance =  df_quar$quartile 


#xtabs(~dep_label+temp,data = df_quar)
```

```{r message=FALSE, warning=FALSE, paged.print=FALSE,r fig1, fig.height = 8, fig.width = 8}
#https://uc-r.github.io/naive_bayes


smp_siz3 = floor(0.75*nrow(df_quar))  
train_ind = sample(seq_len(nrow(df_quar)),size = smp_siz3)
training = df_quar[train_ind,] 
testing = df_quar[-train_ind,] 

model <- naive_bayes(dep_label~origin+carrier+distance+temp+humid+precip+pressure+wind_speed+visib+label,data=training)
predict_r =predict(model,testing, type ='class')


### confusion matrix 
cat("The confusion matrix is")
print(table(predict(model,testing),testing$dep_label))
cat('\n','Accuracy of Naive Bayes',accuracy(table(predict(model,testing),testing$dep_label)),'\n')

### ROC
library(ROCR)
predvec <- ifelse(predict_r==1, 1, 0)
realvec <- ifelse(testing$dep_label==1, 1, 0)
pr <- prediction(predvec, realvec)
prf <- performance(pr, "tpr", "fpr")

plot(perf2,col='coral',main='ROC of Logistic Regression M2')
plot(perf,col='deepskyblue',main='ROC of Logistic Regression M1',add=TRUE)
plot(prf,col = 'mediumorchid',main = 'ROC',add=TRUE)
lines(x = c(0,1), y = c(0,1))
legend(0,1, legend=c("Logistic Regression M1","Logistic Regression M2", "Naive Bayes"),
       col=c("deepskyblue", "coral",'mediumorchid'), lty=1:1, cex=0.8)

```

```{r}
library(ROCR)
### ROC of three models
reg3 <- glm( dep_label~ ., family = binomial(link = "probit"),
	data = training_lg2)

pred.step3 <- predict(reg3, newdata = testing_lg2, type = "response")
pred.class3 <- factor(ifelse(pred.step3 >=0.5, 1, 0))
pred3 <- prediction(pred.step3 ,testing_lg2$dep_label)
perf3 <- performance(pred,"tpr","fpr")

library(pROC)
roc_step <- roc(testing$dep_label, predvec)
roc_step2 <- roc(testing_lg2$dep_label, pred.step2)
roc_step3 <- roc(testing_lg2$dep_label, pred.step3)



plot(perf2,col='royalblue',main='ROC Plot of Prediction Models')
#plot(perf,col='deepskyblue',main='ROC of Logistic Regression M1',add=TRUE)
plot(prf,col = 'mediumorchid',main = 'ROC',add=TRUE)
plot(perf3,col='coral',main='ROC of Logistic Regression M2',add=TRUE)
lines(x = c(0,1), y = c(0,1))
probit = paste('Probit, AUC =',paste(round(roc_step3$auc,3)))
Logist= paste('Logistic Regression, AUC =',paste(round(roc_step2$auc,3)))
Naive = paste('Naive Bayes, AUC =',paste(round(roc_step$auc,3)))
legend(0,1, legend=c(probit,Logist, Naive),
       col=c('coral', "royalblue",'mediumorchid'), lty=1:1, cex=0.7)


```



